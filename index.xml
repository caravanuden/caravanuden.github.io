<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cara Van Uden</title><link>https://caravanuden.com/</link><description>Recent content on Cara Van Uden</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© {year}</copyright><lastBuildDate>Wed, 15 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://caravanuden.com/index.xml" rel="self" type="application/rss+xml"/><item><title>Predicting sustainable development indices from geolocated text</title><link>https://caravanuden.com/projects/sgd-text/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>https://caravanuden.com/projects/sgd-text/</guid><description>tl;dr: In this project, we leverage readily-available natural language data, scraped from Wikipedia, to predict localized indices (asset, sanitation, women&amp;rsquo;s education) relevant to the UN&amp;rsquo;s Sustainability Goals. We explore the impact of different text embedding extraction methods and model architectures on performance in this small data task. We explore logistic regression models, feedforward DNNs, and NLP-CNNs. We use geolocated and extracted “relevant” sentence embeddings to achieve ROC-AUC scores of 0.80 (logistic regression model), 0.</description></item><item><title>Serving ResNet-50 predictions with FastAPI, Redis, and Docker</title><link>https://caravanuden.com/projects/serving-image-classifier/</link><pubDate>Wed, 08 Jul 2020 00:00:00 +0000</pubDate><guid>https://caravanuden.com/projects/serving-image-classifier/</guid><description>tl;dr: This project serves ResNet-50 predictions with FastAPI, Redis, and Docker.
Github repo</description></item><item><title>Google text normalization challenge with LSTM encoder/decoder</title><link>https://caravanuden.com/projects/text-norm/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://caravanuden.com/projects/text-norm/</guid><description>tl;dr: This project takes on text normalization with an LSTM encoder/decoder.
Github repo
Jupyter notebook for the data preprocessing, model training, and model prediction code.
If you want to load my encoder-decoder model, download the hdf5 file and run the following lines of code to play around with my trained model:
import load_model from keras.models
model = load_model(&amp;lsquo;saved_model.hdf5&amp;rsquo;)
If you want to see my predicted results on the 100,000 tokens in the test set, take a look at the &amp;ldquo;test_predictions&amp;rdquo; file.</description></item><item><title>Spotify playlist recommendation challenge with NeuMF</title><link>https://caravanuden.com/projects/spotify-recsys/</link><pubDate>Thu, 31 May 2018 00:00:00 +0000</pubDate><guid>https://caravanuden.com/projects/spotify-recsys/</guid><description>tl;dr: This project takes on playlist recommendation with a neural recommendation system.
Github repo Overview Spotify is an online music streaming service with over 140 million active users and over 30 million tracks. One of its popular features is the ability to create playlists, and the service currently hosts over 2 billion playlists.
This year&amp;rsquo;s challenge focuses on music recommendation, specifically the challenge of automatic playlist continuation. By suggesting appropriate songs to add to a playlist, a recommender system can increase user engagement by making playlist creation easier, as well as extending listening beyond the end of existing playlists.</description></item><item><title>About</title><link>https://caravanuden.com/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://caravanuden.com/about/</guid><description>I&amp;rsquo;m an MS in Computer Science student at Stanford. Here, I&amp;rsquo;m specializing in the Artificial Intelligence track. In the Stanford Machine Learning Group and Shah Lab, I&amp;rsquo;m researching applications of representation learning and few-shot learning using patient images (X-ray and CT) and text (electronic health record).
Before this, I was a machine learning engineer at Wayfair, where I worked on various visual similarity and exact product matching projects.
I graduated from Dartmouth College, where I studied Computer Science and Cognitive Science.</description></item></channel></rss>