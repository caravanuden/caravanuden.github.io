<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>google on Cara Van Uden</title><link>https://caravanuden.com/tags/google/</link><description>Recent content in google on Cara Van Uden</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Â© {year}</copyright><lastBuildDate>Sun, 01 Jul 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://caravanuden.com/tags/google/index.xml" rel="self" type="application/rss+xml"/><item><title>Google text normalization challenge with LSTM encoder/decoder</title><link>https://caravanuden.com/projects/text-norm/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://caravanuden.com/projects/text-norm/</guid><description>tl;dr: This project takes on text normalization with an LSTM encoder/decoder.
Github repo
Jupyter notebook for the data preprocessing, model training, and model prediction code.
If you want to load my encoder-decoder model, download the hdf5 file and run the following lines of code to play around with my trained model:
import load_model from keras.models
model = load_model(&amp;lsquo;saved_model.hdf5&amp;rsquo;)
If you want to see my predicted results on the 100,000 tokens in the test set, take a look at the &amp;ldquo;test_predictions&amp;rdquo; file.</description></item></channel></rss>