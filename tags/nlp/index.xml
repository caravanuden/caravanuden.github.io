<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>nlp on Cara Van Uden</title><link>https://caravanuden.com/tags/nlp/</link><description>Recent content in nlp on Cara Van Uden</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© {year}</copyright><lastBuildDate>Wed, 15 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://caravanuden.com/tags/nlp/index.xml" rel="self" type="application/rss+xml"/><item><title>Predicting sustainable development indices from geolocated text</title><link>https://caravanuden.com/projects/sgd-text/</link><pubDate>Wed, 15 Dec 2021 00:00:00 +0000</pubDate><guid>https://caravanuden.com/projects/sgd-text/</guid><description>tl;dr: In this project, we leverage readily-available natural language data, scraped from Wikipedia, to predict localized indices (asset, sanitation, women&amp;rsquo;s education) relevant to the UN&amp;rsquo;s Sustainability Goals. We explore the impact of different text embedding extraction methods and model architectures on performance in this small data task. We explore logistic regression models, feedforward DNNs, and NLP-CNNs. We use geolocated and extracted “relevant” sentence embeddings to achieve ROC-AUC scores of 0.80 (logistic regression model), 0.</description></item><item><title>Google text normalization challenge with LSTM encoder/decoder</title><link>https://caravanuden.com/projects/text-norm/</link><pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate><guid>https://caravanuden.com/projects/text-norm/</guid><description>tl;dr: This project takes on text normalization with an LSTM encoder/decoder.
Github repo
Jupyter notebook for the data preprocessing, model training, and model prediction code.
If you want to load my encoder-decoder model, download the hdf5 file and run the following lines of code to play around with my trained model:
import load_model from keras.models
model = load_model(&amp;lsquo;saved_model.hdf5&amp;rsquo;)
If you want to see my predicted results on the 100,000 tokens in the test set, take a look at the &amp;ldquo;test_predictions&amp;rdquo; file.</description></item></channel></rss>